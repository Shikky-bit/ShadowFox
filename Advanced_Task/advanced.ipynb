{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30288c41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1 — bash: install libs (run once)\n",
    "!pip -q install transformers==4.35.2 accelerate bitsandbytes datasets evaluate sentence-transformers peft safetensors wandb gradio\n",
    "# Optional: install optimum and onnxruntime if doing ONNX export later\n",
    "!pip -q install optimum[onnx] onnxruntime\n",
    "# Cell 2 — python: mount drive and config\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/llm_project\"  # change if you want\n",
    "import os\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "print(\"base dir:\", BASE_DIR)\n",
    "# Cell 3 — python: set tokens (secure way)\n",
    "# NOTE: Do NOT hardcode tokens in public notebooks. Use colab secrets or input prompts.\n",
    "import getpass\n",
    "HF_TOKEN = getpass.getpass(\"Hugging Face token (or ENTER to skip): \")\n",
    "OPENAI_KEY = getpass.getpass(\"OpenAI key (or ENTER to skip): \")\n",
    "\n",
    "# If you entered HF token, configure env (transformers will pick it up)\n",
    "if HF_TOKEN:\n",
    "    import os\n",
    "    os.environ['HF_HOME'] = os.path.join(BASE_DIR, \".hf\")\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    from huggingface_hub import login\n",
    "    login(token=HF_TOKEN)\n",
    "# Safe Step 4A — auto device, fallback, and helpful warnings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-large\"  # change to smaller if you don't have GPU (e.g., \"google/flan-t5-small\")\n",
    "\n",
    "# detect device\n",
    "if torch.cuda.is_available():\n",
    "    device = 0\n",
    "    device_str = \"cuda\"\n",
    "else:\n",
    "    device = -1\n",
    "    device_str = \"cpu\"\n",
    "\n",
    "print(f\"torch.cuda.is_available() -> {torch.cuda.is_available()}, selected device: {device_str}\")\n",
    "\n",
    "# helper to show GPU info if available\n",
    "import os\n",
    "if device_str == \"cuda\":\n",
    "    try:\n",
    "        print(\"nvidia-smi output:\")\n",
    "        os.system(\"nvidia-smi\")\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"No GPU detected. Inference on CPU will be slow for large models. Consider switching runtime to GPU or using a smaller model (flan-t5-small or flan-t5-base) or API mode.\")\n",
    "\n",
    "print(\"loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"loading model (will use fp16 on CUDA if available)...\")\n",
    "if device_str == \"cuda\":\n",
    "    # try fp16 on GPU\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).half().to(\"cuda\")\n",
    "    inference = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "else:\n",
    "    # CPU fallback (use float32); for big models this may be slow or OOM\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    model.to(\"cpu\")\n",
    "    inference = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "print(\"ready — device:\", device_str)\n",
    "# Safe loader for mac (detects CUDA, MPS (Apple Silicon), or CPU)\n",
    "# Installs small model only to avoid OOMs on CPU/MPS.\n",
    "# NOTE: For MPS, ensure your PyTorch has MPS support (PyTorch 1.12+).\n",
    "\n",
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"mps available:\", getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available())\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "MODEL_NAME = \"google/flan-t5-small\"  # small & friendly for CPU/MPS\n",
    "\n",
    "print(\"Selected model:\", MODEL_NAME)\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Loading model (this may take a moment)...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Select device: prefer cuda > mps > cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Moving model to device:\", device)\n",
    "# NOTE: .half() not recommended on MPS; keep float32 for stability on mac.\n",
    "model.to(device)\n",
    "\n",
    "# simple generation helper that handles device placement\n",
    "def generate(prompt: str, max_new_tokens: int = 128):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        # move inputs to model device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# quick test\n",
    "print(\"\\n-- quick test --\")\n",
    "print(\"Device used:\", device)\n",
    "print(\"Example output:\")\n",
    "print(generate(\"Summarize: Transformers changed NLP because they introduced attention.\", max_new_tokens=50))\n",
    "from datasets import Dataset\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"input\": \"Explain what a language model does in simple words.\",\n",
    "        \"target\": \"A language model predicts the next words or generates text based on patterns it learned.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"input\": \"Summarize: Transformers changed NLP by enabling parallel processing and attention.\",\n",
    "        \"target\": \"Transformers improved NLP by using attention and processing text in parallel.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"input\": \"Paraphrase: Machine learning helps computers learn patterns from data.\",\n",
    "        \"target\": \"Machine learning allows computers to discover patterns from examples.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "ds = Dataset.from_list(data)\n",
    "ds\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for row in ds:\n",
    "    prompt = row[\"input\"]\n",
    "    print(\"\\nPROMPT:\", prompt)\n",
    "\n",
    "    pred = generate(prompt)\n",
    "    print(\"MODEL OUTPUT:\", pred)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(row[\"target\"])\n",
    "\n",
    "print(\"\\nAll predictions done.\")\n",
    "!pip -q install evaluate sentence-transformers\n",
    "!pip install rouge_score\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "bleu_score = bleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=[[ref] for ref in references]\n",
    ")\n",
    "\n",
    "rouge_scores = rouge.compute(\n",
    "    predictions=predictions,\n",
    "    references=references\n",
    ")\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "emb_pred = embedder.encode(predictions, convert_to_tensor=True)\n",
    "emb_ref = embedder.encode(references, convert_to_tensor=True)\n",
    "\n",
    "semantic_scores = util.cos_sim(emb_pred, emb_ref).diag().cpu().tolist()\n",
    "\n",
    "print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "print(\"BLEU:\", bleu_score)\n",
    "print(\"ROUGE:\", {k: rouge_scores[k] for k in ['rouge1','rouge2','rougeL']})\n",
    "print(\"Semantic similarity:\", semantic_scores)\n",
    "# Visualization for model eval results\n",
    "# Requirements: matplotlib, rouge_score, sentence-transformers\n",
    "# Install if missing (uncomment to run)\n",
    "# !pip install matplotlib rouge_score sentence-transformers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# ---- INPUTS (should already exist from your pipeline) ----\n",
    "# predictions: list of strings (model outputs)\n",
    "# references:  list of strings (gold targets)\n",
    "# If semantic_scores already computed, you can set semantic_scores = [...] here.\n",
    "# Otherwise we'll compute it below.\n",
    "\n",
    "# sanity check\n",
    "try:\n",
    "    preds = predictions\n",
    "    refs = references\n",
    "except NameError:\n",
    "    raise RuntimeError(\"You must run the inference step first so `predictions` and `references` exist.\")\n",
    "\n",
    "if len(preds) != len(refs):\n",
    "    raise RuntimeError(\"predictions and references must have the same length.\")\n",
    "\n",
    "# ---- compute semantic similarity per example (if not present) ----\n",
    "try:\n",
    "    semantic_scores\n",
    "except NameError:\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    emb_pred = embedder.encode(preds, convert_to_tensor=True)\n",
    "    emb_ref = embedder.encode(refs, convert_to_tensor=True)\n",
    "    semantic_scores = util.cos_sim(emb_pred, emb_ref).diag().cpu().tolist()\n",
    "\n",
    "# ---- compute per-example ROUGE-1 (F1) using rouge_score ----\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
    "rouge1_f1 = []\n",
    "rouge2_f1 = []\n",
    "rougeL_f1 = []\n",
    "for p, r in zip(preds, refs):\n",
    "    scores = scorer.score(r, p)   # note: rouge scorer signature is (target, prediction)\n",
    "    rouge1_f1.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_f1.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_f1.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# ---- compute aggregate BLEU & ROUGE via evaluate if available (optional) ----\n",
    "agg_bleu = None\n",
    "agg_rouge = None\n",
    "try:\n",
    "    import evaluate\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    agg_bleu = bleu.compute(predictions=preds, references=[[r] for r in refs])\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    agg_rouge = rouge.compute(predictions=preds, references=refs)\n",
    "except Exception:\n",
    "    # evaluate may not be installed or missing extras; we already have per-example rouge from rouge_score\n",
    "    pass\n",
    "\n",
    "# ---- plotting ----\n",
    "indices = np.arange(len(preds)).astype(int)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(indices, rouge1_f1, marker='o')\n",
    "plt.title(\"Per-example ROUGE-1 (F1) across dataset\")\n",
    "plt.xlabel(\"Example index\")\n",
    "plt.ylabel(\"ROUGE-1 F1\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(indices, semantic_scores, marker='o')\n",
    "plt.title(\"Per-example Semantic Similarity (cosine)\")\n",
    "plt.xlabel(\"Example index\")\n",
    "plt.ylabel(\"Cosine similarity\")\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# combined bar for ROUGE1 vs Semantic\n",
    "width = 0.35\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(indices - width/2, rouge1_f1, width=width)\n",
    "plt.bar(indices + width/2, semantic_scores, width=width)\n",
    "plt.title(\"ROUGE-1 (F1) vs Semantic Similarity per example\")\n",
    "plt.xlabel(\"Example index\")\n",
    "plt.legend([\"ROUGE-1 F1\",\"Semantic sim\"])\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- print summary stats ----\n",
    "def pct(x):\n",
    "    return f\"{x*100:.2f}%\"\n",
    "\n",
    "print(\"\\n=== Aggregate metrics ===\")\n",
    "if agg_bleu is not None:\n",
    "    try:\n",
    "        print(\"BLEU:\", agg_bleu)\n",
    "    except Exception:\n",
    "        print(\"BLEU computed but couldn't format.\")\n",
    "else:\n",
    "    print(\"BLEU: (not computed — install `evaluate` to compute overall BLEU)\")\n",
    "\n",
    "if agg_rouge is not None:\n",
    "    print(\"ROUGE (aggregate):\")\n",
    "    # print some keys if present\n",
    "    for k in ['rouge1','rouge2','rougeL']:\n",
    "        if k in agg_rouge:\n",
    "            print(f\"  {k}:\", agg_rouge[k])\n",
    "else:\n",
    "    # fallback: print mean per-example rouge1\n",
    "    mean_r1 = sum(rouge1_f1)/len(rouge1_f1) if len(rouge1_f1) else float('nan')\n",
    "    print(\"ROUGE (mean per-example):\")\n",
    "    print(\"  ROUGE-1 (mean F1):\", f\"{mean_r1:.4f}\", pct(mean_r1))\n",
    "\n",
    "mean_sem = sum(semantic_scores)/len(semantic_scores) if len(semantic_scores) else float('nan')\n",
    "print(\"Mean semantic similarity:\", f\"{mean_sem:.4f}\", pct(mean_sem))\n",
    "\n",
    "# ---- save figures (optional) ----\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(indices, semantic_scores, marker='o')\n",
    "plt.title(\"Semantic similarity\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"semantic_similarity.png\", dpi=150)\n",
    "print(\"\\nSaved example figure: semantic_similarity.png\")\n",
    "# Cell: install required packages (run if not installed)\n",
    "# NOTE: In Colab uncomment the installs. On local mac, install once via pip.\n",
    "# !pip install -q transformers==4.35.2 sentence-transformers evaluate rouge_score matplotlib datasets\n",
    "\n",
    "# Imports\n",
    "import os, sys, time, math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "# Device detection (works on mac MPS, Colab CUDA, or CPU)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "mps_avail = getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available()\n",
    "print(\"cuda:\", cuda_avail, \"mps:\", mps_avail)\n",
    "\n",
    "device = \"cpu\"\n",
    "if cuda_avail:\n",
    "    device = \"cuda\"\n",
    "elif mps_avail:\n",
    "    device = \"mps\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL = \"google/flan-t5-small\"\n",
    "print(\"Loading\", MODEL, \"...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "# move model to device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded.\")\n",
    "# Simple generate helper\n",
    "def generate_text(prompt, max_new_tokens=128):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "data = [\n",
    "    {\"id\":\"q1\", \"task\":\"summarize\", \"input\":\"Summarize: Transformers changed NLP by enabling attention and parallel processing.\" , \"target\":\"Transformers improved NLP through attention and parallel processing.\"},\n",
    "    {\"id\":\"q2\", \"task\":\"paraphrase\", \"input\":\"Paraphrase: What does a language model do?\", \"target\":\"A language model predicts or generates text by learning patterns from data.\"},\n",
    "    {\"id\":\"q3\", \"task\":\"qa\", \"input\":\"Q: What is overfitting? A:\", \"target\":\"Overfitting occurs when a model learns noise and performs poorly on new data.\"},\n",
    "    {\"id\":\"q4\", \"task\":\"context\", \"input\":\"Given: Alice went to the store. She bought apples. Question: Who bought apples?\", \"target\":\"Alice bought apples.\"},\n",
    "    {\"id\":\"q5\", \"task\":\"creativity\", \"input\":\"Write a short, creative two-sentence future-tech pitch about energy-saving drones.\", \"target\":\"<creative - freeform>\"},\n",
    "    {\"id\":\"q6\", \"task\":\"domain\", \"input\":\"Explain 'backpropagation' in simple words.\", \"target\":\"Backpropagation updates model weights by propagating error gradients backward through the network.\"},\n",
    "]\n",
    "ds = Dataset.from_list(data)\n",
    "ds\n",
    "preds = []\n",
    "refs = []\n",
    "meta = []\n",
    "\n",
    "print(\"Running inference on dataset...\")\n",
    "for item in ds:\n",
    "    prompt = item[\"input\"]\n",
    "    # If summarization/paraphrase tasks, give explicit instruction (FLAN-T5 benefits from instruction prompts)\n",
    "    if item[\"task\"] == \"summarize\":\n",
    "        full_prompt = f\"summarize: {prompt.split(':',1)[1].strip()}\"\n",
    "    elif item[\"task\"] == \"paraphrase\":\n",
    "        full_prompt = f\"paraphrase: {prompt.split(':',1)[1].strip()}\"\n",
    "    else:\n",
    "        full_prompt = prompt\n",
    "\n",
    "    out = generate_text(full_prompt, max_new_tokens=64)\n",
    "    preds.append(out)\n",
    "    refs.append(item[\"target\"])\n",
    "    meta.append({\"id\":item[\"id\"], \"task\": item[\"task\"], \"prompt\": full_prompt, \"output\": out})\n",
    "    print(f\"\\n[{item['id']} - {item['task']}]\")\n",
    "    print(\"PROMPT:\", full_prompt)\n",
    "    print(\"OUTPUT: \", out)\n",
    "    print(\"TARGET:\", item[\"target\"])\n",
    "print(\"\\nDone.\")\n",
    "# Multi-turn / context probe\n",
    "multi_prompt = \"\"\"System: You are a helpful assistant.\n",
    "User: John gave Mary a book. Later John asked Mary to return it.\n",
    "User: Who originally gave the book?\n",
    "Answer:\"\"\"\n",
    "print(\"Multi-turn context probe:\\n\", generate_text(multi_prompt, max_new_tokens=40))\n",
    "\n",
    "# Prompt sensitivity: two near-identical prompts\n",
    "p1 = \"Explain backpropagation in simple words.\"\n",
    "p2 = \"In simple words, what is backpropagation?\"\n",
    "print(\"\\nPrompt A:\", p1, \"\\n->\", generate_text(\"explain: \" + p1))\n",
    "print(\"\\nPrompt B:\", p2, \"\\n->\", generate_text(\"explain: \" + p2))\n",
    "# Compute BLEU (via evaluate), ROUGE per example (rouge_score), and semantic similarity via SBERT\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "# compute BLEU:\n",
    "bleu_res = bleu.compute(predictions=preds, references=[[r] for r in refs])\n",
    "print(\"BLEU:\", bleu_res)\n",
    "\n",
    "# ROUGE per example using rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
    "rouge1_f1 = []\n",
    "rougeL_f1 = []\n",
    "for p,r in zip(preds, refs):\n",
    "    s = scorer.score(r, p)\n",
    "    rouge1_f1.append(s['rouge1'].fmeasure)\n",
    "    rougeL_f1.append(s['rougeL'].fmeasure)\n",
    "\n",
    "print(\"Per-example ROUGE-1 F1:\", rouge1_f1)\n",
    "\n",
    "# Semantic similarity\n",
    "embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "emb_p = embed.encode(preds, convert_to_tensor=True)\n",
    "emb_r = embed.encode(refs, convert_to_tensor=True)\n",
    "sims = util.cos_sim(emb_p, emb_r).diag().cpu().tolist()\n",
    "print(\"Semantic similarities:\", sims)\n",
    "indices = list(range(len(preds)))\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(indices, rouge1_f1, marker='o', label='ROUGE-1 F1')\n",
    "plt.plot(indices, sims, marker='x', label='Semantic sim (cosine)')\n",
    "plt.title('Per-example performance: ROUGE-1 vs Semantic sim')\n",
    "plt.xlabel('Example index')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
